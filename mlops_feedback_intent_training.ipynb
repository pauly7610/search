{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# MLOps Feedback Intent Training Pipeline\n",
    "\n",
    "This notebook implements a complete MLOps pipeline for training intent classifiers using feedback data from the search platform.\n",
    "\n",
    "## Features:\n",
    "- Data export from backend APIs\n",
    "- Feature engineering and preprocessing\n",
    "- Model training with scikit-learn\n",
    "- MLflow experiment tracking\n",
    "- Model evaluation and metrics\n",
    "- Automated retraining capabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'backend_url': 'http://localhost:8000',\n",
    "    'mlflow_tracking_uri': 'http://localhost:5000',  # MLflow server\n",
    "    'experiment_name': 'intent_classification',\n",
    "    'model_name': 'feedback_intent_classifier',\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'cv_folds': 5\n",
    "}\n",
    "\n",
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(CONFIG['mlflow_tracking_uri'])\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_training_data(backend_url: str, days_back: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch training data from backend APIs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate date range\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=days_back)\n",
    "        \n",
    "        # Fetch feedback data with message content\n",
    "        feedback_url = f\"{backend_url}/api/feedback/export\"\n",
    "        params = {\n",
    "            'format': 'json',\n",
    "            'start_date': start_date.isoformat(),\n",
    "            'end_date': end_date.isoformat()\n",
    "        }\n",
    "        \n",
    "        print(f\"Fetching feedback data from {feedback_url}\")\n",
    "        response = requests.get(feedback_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        feedback_data = response.json()\n",
    "        df = pd.DataFrame(feedback_data)\n",
    "        \n",
    "        print(f\"Fetched {len(df)} feedback records\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Fetch the data\n",
    "training_data = fetch_training_data(CONFIG['backend_url'])\n",
    "print(f\"Training data shape: {training_data.shape}\")\n",
    "if not training_data.empty:\n",
    "    print(f\"Columns: {list(training_data.columns)}\")\n",
    "    print(f\"Sample data:\")\n",
    "    print(training_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Preprocess the training data for intent classification\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df, {}\n",
    "    \n",
    "    # Data cleaning\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Remove null values\n",
    "    df_clean = df_clean.dropna(subset=['message_content', 'rating'])\n",
    "    \n",
    "    # Create intent labels based on feedback patterns\n",
    "    # This is a simplified approach - in practice, you'd have more sophisticated labeling\n",
    "    def create_intent_label(row):\n",
    "        content = str(row['message_content']).lower()\n",
    "        rating = row['rating']\n",
    "        \n",
    "        # Simple rule-based intent classification for training\n",
    "        if 'bill' in content or 'payment' in content or 'charge' in content:\n",
    "            return 'billing'\n",
    "        elif 'internet' in content or 'wifi' in content or 'connection' in content:\n",
    "            return 'technical_support'\n",
    "        elif 'channel' in content or 'tv' in content or 'cable' in content:\n",
    "            return 'tv_support'\n",
    "        elif 'account' in content or 'profile' in content or 'login' in content:\n",
    "            return 'account_management'\n",
    "        elif 'service' in content or 'plan' in content or 'upgrade' in content:\n",
    "            return 'service_inquiry'\n",
    "        else:\n",
    "            return 'general_inquiry'\n",
    "    \n",
    "    df_clean['intent'] = df_clean.apply(create_intent_label, axis=1)\n",
    "    \n",
    "    # Feature engineering\n",
    "    df_clean['message_length'] = df_clean['message_content'].str.len()\n",
    "    df_clean['word_count'] = df_clean['message_content'].str.split().str.len()\n",
    "    df_clean['has_question'] = df_clean['message_content'].str.contains('\\\\?').astype(int)\n",
    "    \n",
    "    # Create satisfaction labels\n",
    "    df_clean['satisfaction'] = df_clean['rating'].apply(\n",
    "        lambda x: 'positive' if x >= 4 else 'negative' if x <= 2 else 'neutral'\n",
    "    )\n",
    "    \n",
    "    # Statistics\n",
    "    stats = {\n",
    "        'total_samples': len(df_clean),\n",
    "        'intent_distribution': df_clean['intent'].value_counts().to_dict(),\n",
    "        'satisfaction_distribution': df_clean['satisfaction'].value_counts().to_dict(),\n",
    "        'avg_message_length': df_clean['message_length'].mean(),\n",
    "        'avg_word_count': df_clean['word_count'].mean()\n",
    "    }\n",
    "    \n",
    "    print(\"Data preprocessing completed!\")\n",
    "    print(f\"Clean samples: {len(df_clean)}\")\n",
    "    print(f\"Intent distribution: {stats['intent_distribution']}\")\n",
    "    print(f\"Satisfaction distribution: {stats['satisfaction_distribution']}\")\n",
    "    \n",
    "    return df_clean, stats\n",
    "\n",
    "# Preprocess the data\n",
    "if not training_data.empty:\n",
    "    processed_data, data_stats = preprocess_data(training_data)\n",
    "else:\n",
    "    print(\"No training data available - using sample data for demonstration\")\n",
    "    # Create sample data for demonstration\n",
    "    sample_data = pd.DataFrame({\n",
    "        'message_content': [\n",
    "            'My internet is not working properly',\n",
    "            'I need help with my bill payment',\n",
    "            'TV channels are not loading',\n",
    "            'How to change my account password',\n",
    "            'Want to upgrade my service plan',\n",
    "            'General question about services'\n",
    "        ] * 10,  # Repeat for more samples\n",
    "        'rating': [1, 5, 2, 4, 5, 3] * 10,\n",
    "        'created_at': [datetime.now() - timedelta(days=i) for i in range(60)]\n",
    "    })\n",
    "    processed_data, data_stats = preprocess_data(sample_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data distribution\n",
    "if not processed_data.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Intent distribution\n",
    "    intent_counts = processed_data['intent'].value_counts()\n",
    "    axes[0, 0].bar(intent_counts.index, intent_counts.values)\n",
    "    axes[0, 0].set_title('Intent Distribution')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Satisfaction distribution\n",
    "    satisfaction_counts = processed_data['satisfaction'].value_counts()\n",
    "    axes[0, 1].pie(satisfaction_counts.values, labels=satisfaction_counts.index, autopct='%1.1f%%')\n",
    "    axes[0, 1].set_title('Satisfaction Distribution')\n",
    "    \n",
    "    # Message length distribution\n",
    "    axes[1, 0].hist(processed_data['message_length'], bins=20, alpha=0.7)\n",
    "    axes[1, 0].set_title('Message Length Distribution')\n",
    "    axes[1, 0].set_xlabel('Message Length')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Rating vs Intent\n",
    "    rating_intent = processed_data.groupby(['intent', 'rating']).size().unstack(fill_value=0)\n",
    "    sns.heatmap(rating_intent, annot=True, fmt='d', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Rating vs Intent Heatmap')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Data visualization completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ml_pipeline() -> Pipeline:\n",
    "    \"\"\"\n",
    "    Create the ML pipeline for intent classification\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            max_features=10000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english',\n",
    "            lowercase=True,\n",
    "            strip_accents='ascii'\n",
    "        )),\n",
    "        ('classifier', LogisticRegression(\n",
    "            random_state=CONFIG['random_state'],\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced'\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def train_model(df: pd.DataFrame) -> Tuple[Pipeline, Dict]:\n",
    "    \"\"\"\n",
    "    Train the intent classification model\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No data available for training\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = df['message_content']\n",
    "    y = df['intent']\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=CONFIG['test_size'], \n",
    "        random_state=CONFIG['random_state'],\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {len(X_train)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "    \n",
    "    # Create and train the model\n",
    "    pipeline = create_ml_pipeline()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        pipeline, X_train, y_train, \n",
    "        cv=CONFIG['cv_folds'], \n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'classification_report': classification_report(y_test, y_pred, output_dict=True),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),\n",
    "        'feature_count': pipeline.named_steps['tfidf'].get_feature_names_out().shape[0]\n",
    "    }\n",
    "    \n",
    "    print(f\"Model trained successfully!\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"CV Mean Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    return pipeline, metrics\n",
    "\n",
    "# Train the model\n",
    "if not processed_data.empty:\n",
    "    model, training_metrics = train_model(processed_data)\n",
    "    print(\"\\\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        processed_data['intent'], \n",
    "        model.predict(processed_data['message_content'])\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow Experiment Tracking\n",
    "def log_experiment(model: Pipeline, metrics: Dict, data_stats: Dict):\n",
    "    \"\"\"\n",
    "    Log the experiment to MLflow\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set or create experiment\n",
    "        experiment = mlflow.get_experiment_by_name(CONFIG['experiment_name'])\n",
    "        if experiment is None:\n",
    "            experiment_id = mlflow.create_experiment(CONFIG['experiment_name'])\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "        \n",
    "        with mlflow.start_run(experiment_id=experiment_id):\n",
    "            # Log parameters\n",
    "            mlflow.log_param(\"model_type\", \"logistic_regression_tfidf\")\n",
    "            mlflow.log_param(\"test_size\", CONFIG['test_size'])\n",
    "            mlflow.log_param(\"cv_folds\", CONFIG['cv_folds'])\n",
    "            mlflow.log_param(\"random_state\", CONFIG['random_state'])\n",
    "            mlflow.log_param(\"total_samples\", data_stats['total_samples'])\n",
    "            mlflow.log_param(\"feature_count\", metrics['feature_count'])\n",
    "            \n",
    "            # Log TF-IDF parameters\n",
    "            tfidf_params = model.named_steps['tfidf'].get_params()\n",
    "            for param, value in tfidf_params.items():\n",
    "                mlflow.log_param(f\"tfidf_{param}\", value)\n",
    "            \n",
    "            # Log classifier parameters\n",
    "            clf_params = model.named_steps['classifier'].get_params()\n",
    "            for param, value in clf_params.items():\n",
    "                mlflow.log_param(f\"classifier_{param}\", value)\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"accuracy\", metrics['accuracy'])\n",
    "            mlflow.log_metric(\"cv_mean_accuracy\", metrics['cv_mean'])\n",
    "            mlflow.log_metric(\"cv_std_accuracy\", metrics['cv_std'])\n",
    "            \n",
    "            # Log per-class metrics\n",
    "            for intent, class_metrics in metrics['classification_report'].items():\n",
    "                if isinstance(class_metrics, dict):\n",
    "                    for metric_name, value in class_metrics.items():\n",
    "                        if isinstance(value, (int, float)):\n",
    "                            mlflow.log_metric(f\"{intent}_{metric_name}\", value)\n",
    "            \n",
    "            # Log data distribution\n",
    "            for intent, count in data_stats['intent_distribution'].items():\n",
    "                mlflow.log_metric(f\"intent_count_{intent}\", count)\n",
    "            \n",
    "            # Log model\n",
    "            mlflow.sklearn.log_model(\n",
    "                model, \n",
    "                CONFIG['model_name'],\n",
    "                registered_model_name=CONFIG['model_name']\n",
    "            )\n",
    "            \n",
    "            # Log confusion matrix as artifact\n",
    "            cm_df = pd.DataFrame(\n",
    "                metrics['confusion_matrix'],\n",
    "                index=model.classes_,\n",
    "                columns=model.classes_\n",
    "            )\n",
    "            cm_df.to_csv(\"confusion_matrix.csv\")\n",
    "            mlflow.log_artifact(\"confusion_matrix.csv\")\n",
    "            \n",
    "            # Log feature importance (top TF-IDF features)\n",
    "            feature_names = model.named_steps['tfidf'].get_feature_names_out()\n",
    "            feature_importance = np.abs(model.named_steps['classifier'].coef_).mean(axis=0)\n",
    "            top_features = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': feature_importance\n",
    "            }).nlargest(50, 'importance')\n",
    "            top_features.to_csv(\"top_features.csv\", index=False)\n",
    "            mlflow.log_artifact(\"top_features.csv\")\n",
    "            \n",
    "            run_id = mlflow.active_run().info.run_id\n",
    "            print(f\"Experiment logged successfully! Run ID: {run_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error logging to MLflow: {e}\")\n",
    "        print(\"Continuing without MLflow logging...\")\n",
    "\n",
    "# Log the experiment\n",
    "if not processed_data.empty and 'model' in locals():\n",
    "    log_experiment(model, training_metrics, data_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Visualization\n",
    "def evaluate_model(model: Pipeline, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with visualizations\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return\n",
    "    \n",
    "    X = df['message_content']\n",
    "    y = df['intent']\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_proba = model.predict_proba(X)\n",
    "    \n",
    "    # Create evaluation plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', \n",
    "                xticklabels=model.classes_, \n",
    "                yticklabels=model.classes_,\n",
    "                ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Confusion Matrix')\n",
    "    axes[0, 0].set_xlabel('Predicted')\n",
    "    axes[0, 0].set_ylabel('Actual')\n",
    "    \n",
    "    # Feature Importance (Top 20)\n",
    "    feature_names = model.named_steps['tfidf'].get_feature_names_out()\n",
    "    feature_importance = np.abs(model.named_steps['classifier'].coef_).mean(axis=0)\n",
    "    top_indices = np.argsort(feature_importance)[-20:]\n",
    "    \n",
    "    axes[0, 1].barh(range(20), feature_importance[top_indices])\n",
    "    axes[0, 1].set_yticks(range(20))\n",
    "    axes[0, 1].set_yticklabels([feature_names[i] for i in top_indices])\n",
    "    axes[0, 1].set_title('Top 20 Features')\n",
    "    axes[0, 1].set_xlabel('Importance')\n",
    "    \n",
    "    # Prediction Confidence Distribution\n",
    "    max_probas = np.max(y_pred_proba, axis=1)\n",
    "    axes[1, 0].hist(max_probas, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].set_title('Prediction Confidence Distribution')\n",
    "    axes[1, 0].set_xlabel('Max Probability')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].axvline(np.mean(max_probas), color='red', linestyle='--', \n",
    "                       label=f'Mean: {np.mean(max_probas):.3f}')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Per-class Performance\n",
    "    report = classification_report(y, y_pred, output_dict=True)\n",
    "    classes = [c for c in report.keys() if c not in ['accuracy', 'macro avg', 'weighted avg']]\n",
    "    precision_scores = [report[c]['precision'] for c in classes]\n",
    "    recall_scores = [report[c]['recall'] for c in classes]\n",
    "    f1_scores = [report[c]['f1-score'] for c in classes]\n",
    "    \n",
    "    x = np.arange(len(classes))\n",
    "    width = 0.25\n",
    "    \n",
    "    axes[1, 1].bar(x - width, precision_scores, width, label='Precision', alpha=0.8)\n",
    "    axes[1, 1].bar(x, recall_scores, width, label='Recall', alpha=0.8)\n",
    "    axes[1, 1].bar(x + width, f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Intent Classes')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].set_title('Per-Class Performance Metrics')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels(classes, rotation=45)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed metrics\n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "    print(\"DETAILED MODEL EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Overall Accuracy: {accuracy_score(y, y_pred):.4f}\")\n",
    "    print(f\"Average Confidence: {np.mean(max_probas):.4f}\")\n",
    "    print(f\"Low Confidence Predictions (<0.7): {np.sum(max_probas < 0.7)} ({np.sum(max_probas < 0.7)/len(max_probas)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\\\nClassification Report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "# Evaluate the model\n",
    "if not processed_data.empty and 'model' in locals():\n",
    "    evaluate_model(model, processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inference and Testing\n",
    "def predict_intent(model: Pipeline, message: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Predict intent for a single message\n",
    "    \"\"\"\n",
    "    prediction = model.predict([message])[0]\n",
    "    probabilities = model.predict_proba([message])[0]\n",
    "    \n",
    "    # Create probability dictionary\n",
    "    prob_dict = {\n",
    "        intent: float(prob) \n",
    "        for intent, prob in zip(model.classes_, probabilities)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'message': message,\n",
    "        'predicted_intent': prediction,\n",
    "        'confidence': float(max(probabilities)),\n",
    "        'all_probabilities': prob_dict\n",
    "    }\n",
    "\n",
    "# Test the model with sample messages\n",
    "if 'model' in locals():\n",
    "    test_messages = [\n",
    "        \"My internet is very slow today\",\n",
    "        \"I can't pay my bill online\",\n",
    "        \"The TV channels are not working\",\n",
    "        \"How do I reset my password?\",\n",
    "        \"I want to upgrade my service\",\n",
    "        \"What are your business hours?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"MODEL INFERENCE TESTING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for message in test_messages:\n",
    "        result = predict_intent(model, message)\n",
    "        print(f\"\\\\nMessage: '{result['message']}'\")\n",
    "        print(f\"Predicted Intent: {result['predicted_intent']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "        print(\"Top 3 Probabilities:\")\n",
    "        sorted_probs = sorted(result['all_probabilities'].items(), \n",
    "                            key=lambda x: x[1], reverse=True)[:3]\n",
    "        for intent, prob in sorted_probs:\n",
    "            print(f\"  {intent}: {prob:.4f}\")\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Automated Retraining Pipeline\n",
    "\n",
    "The following section demonstrates how to set up automated retraining using Airflow or similar orchestration tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated Retraining Functions\n",
    "def should_retrain(backend_url: str, threshold_days: int = 7) -> bool:\n",
    "    \"\"\"\n",
    "    Determine if model should be retrained based on new data availability\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check for new feedback data\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=threshold_days)\n",
    "        \n",
    "        response = requests.get(f\"{backend_url}/api/feedback/export\", params={\n",
    "            'format': 'json',\n",
    "            'start_date': start_date.isoformat(),\n",
    "            'end_date': end_date.isoformat()\n",
    "        })\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            new_data = response.json()\n",
    "            return len(new_data) > 10  # Retrain if more than 10 new samples\n",
    "        \n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking for new data: {e}\")\n",
    "        return False\n",
    "\n",
    "def automated_training_pipeline():\n",
    "    \"\"\"\n",
    "    Complete automated training pipeline\n",
    "    \"\"\"\n",
    "    print(\"Starting automated training pipeline...\")\n",
    "    \n",
    "    # Check if retraining is needed\n",
    "    if not should_retrain(CONFIG['backend_url']):\n",
    "        print(\"No retraining needed - insufficient new data\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Fetch fresh data\n",
    "        print(\"Fetching fresh training data...\")\n",
    "        fresh_data = fetch_training_data(CONFIG['backend_url'], days_back=60)\n",
    "        \n",
    "        if fresh_data.empty:\n",
    "            print(\"No data available for training\")\n",
    "            return\n",
    "        \n",
    "        # Preprocess data\n",
    "        print(\"Preprocessing data...\")\n",
    "        processed_data, stats = preprocess_data(fresh_data)\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Training model...\")\n",
    "        model, metrics = train_model(processed_data)\n",
    "        \n",
    "        # Log to MLflow\n",
    "        print(\"Logging to MLflow...\")\n",
    "        log_experiment(model, metrics, stats)\n",
    "        \n",
    "        # Save model locally\n",
    "        import joblib\n",
    "        model_filename = f\"intent_classifier_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "        joblib.dump(model, model_filename)\n",
    "        print(f\"Model saved as {model_filename}\")\n",
    "        \n",
    "        print(\"Automated training pipeline completed successfully!\")\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'model_file': model_filename,\n",
    "            'metrics': metrics,\n",
    "            'data_stats': stats\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in automated training pipeline: {e}\")\n",
    "        return {'success': False, 'error': str(e)}\n",
    "\n",
    "# Example of running automated pipeline\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"AUTOMATED RETRAINING PIPELINE DEMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simulate automated retraining check\n",
    "retrain_needed = should_retrain(CONFIG['backend_url'])\n",
    "print(f\"Retraining needed: {retrain_needed}\")\n",
    "\n",
    "if retrain_needed:\n",
    "    print(\"Running automated training pipeline...\")\n",
    "    # result = automated_training_pipeline()\n",
    "    print(\"(Pipeline would run here in production)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook provides a complete MLOps pipeline for intent classification:\n",
    "\n",
    "### What we've built:\n",
    "1. **Data Pipeline**: Automated data fetching from backend APIs\n",
    "2. **Feature Engineering**: Text preprocessing and intent labeling\n",
    "3. **Model Training**: Scikit-learn pipeline with TF-IDF and Logistic Regression\n",
    "4. **Experiment Tracking**: MLflow integration for reproducibility\n",
    "5. **Model Evaluation**: Comprehensive metrics and visualizations\n",
    "6. **Automated Retraining**: Pipeline for continuous model improvement\n",
    "\n",
    "### Production Deployment:\n",
    "- Set up MLflow server for experiment tracking\n",
    "- Create Airflow DAGs for scheduled retraining\n",
    "- Implement model serving endpoints\n",
    "- Add model monitoring and drift detection\n",
    "- Set up automated alerts for model performance degradation\n",
    "\n",
    "### Model Improvements:\n",
    "- Experiment with different algorithms (Random Forest, XGBoost, Neural Networks)\n",
    "- Implement advanced NLP techniques (BERT, sentence transformers)\n",
    "- Add active learning for better label quality\n",
    "- Implement ensemble methods for better performance\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
